# Chapter 2: Cross Validation

#machine_learning 

## Cross Validation: Main Ideas

- The Problem
	- "How do we pick the best points for training data and best points for testing data?"
- A Solution
	- Use cross validation to figure out which is which
	- Cross validation uses *all* points for figuring out which points are best for which in an *iterative* way

## Cross Validation: Details (Part 1)

- Using all the data points for training and testing purposes...
	- ... is a terrible idea because the only way to determine if a machine learning method has been **overfit** to the training data is to try it on a new data that it hasn't encountered
- Data Leakage
	- Reusing the same data for training and testing purposes
	- Results in you believing that the machine learning method will perform better than it does because it will **overfit**

## Cross Validation: Details (Part 2)

- "What if we randomly select some data for testing and the rest for training?"
	- Avoids data leakage, but there's no way to verify that the data points we selected are the best data for training purposes
- Cross Validation
	- Solves the problem of not knowing which points are best for testing by using them *all* iteratively
	- Steps
		- Randomly assign data to different groups (e.g. 5 folds)
		- Train your data on a subset of the data (e.g. 4 folds), test on remaining fold
		- Measure errors for each point in the **testing** data
		- Rinse & repeat so that all the other folds get a chance to be used as the testing set

## Cross Validation: Details (Part 3)

- Cross Validation (cont.)
	- Steps
		- Each iteration will produce different performances and results
			- Graphically speaking, you should see different fitted lines for each iteration, giving us different prediction errors
		- Average these errors to get a sense of how well this model will perform with other (future) data
	- Cross validation ensures that your machine learning model isn't just good at memorization, it helps its ability to generalize in *new* situations, making it more reliable for performance measurements

## Cross Validation: Details (Part 4)

- By using cross validation, you can compare the errors generated by different methods to quantify which method performs better than its counterpart, **without** having to worry about whether or not we selected the best data for training and testing purposes

## Cross Validation: Details (Part 5)

- 10- fold CV is commonly used when there's a lot of data

## Cross Validation: Details (Part 6)

- Leave- One- Out Cross Validation (LOOCV)
	- Use all but one point for **training** and use the remaining point for **testing**
	- Iterate until all points have ben used for testing
	- Best practice is to use LOOCV when the dataset is small, 10- fold CV when the dataset is large

## Cross Validation: Details (Part 7)

- Oftentimes, when we use CV to compare machine learning methods' performances, the performance isn't one sided for every iteration and additional analysis is needed to conclude which method is better conclusively