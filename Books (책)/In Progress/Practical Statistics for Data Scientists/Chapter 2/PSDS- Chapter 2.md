# Chapter 2: Data and Sampling Distributions

#data_science #statistics 

## Random Sampling and Sampling Bias

- Sampling
	- In the era of big data, there is **still** a need for sampling
	- Proliferation of data of varying quality and relevance reinforces the need for sampling as a tool to work efficiently with a variety of data to minimize bias
- Traditional vs. Modern Statistics
	- Traditional statistics focused on the population, that is assumed to follow an underlying but *unknown* distribution, using theory based on strong assumptions about the population itself
	- Modern statistics focuses on everything that is available in the *sample* data and its empirical distribution, where such assumptions are not needed
	- *Generally* speaking, data scientists do not concern themselves too much with traditional statistics and instead focus on sampling procedures and the data at hand
		- Exceptions to this rule arise when data is generated from a physical process that can be modeled directly
			- e.g. coin flip simulation, which follows a binomial distribution
			- Binomial situations can be modeled effectively by the coin example (with a modification to the probability of landing a certain outcome), in which case additional insight using our understanding of the population proves to be crucial
- Terminology
	- Sample
		- Subset of data from a larger dataset (population)
		- Populations in statistics is a large, defined (sometimes theoretical) set of data
	- Random Sampling
		- Process in which each available member of the population that is being sampled has an *equal* chance of being chosen for the sample at *each* draw, which results in a *simple random sample*
		- *Sampling with replacement* indicates that observations are replaced back into the population after each draw, indicating a possibility of reselection
			- *Sampling without replacement* indicates the impossibility of reselection after each draw
	- Representativeness
		- Statistical notion of quality of the data itself, which encompasses completeness, consistency, cleanliness and accuracy 
	- Sample Bias
		- Indication that the sample was different in a significant, *nonrandom* way from the original population it was meant to represent
		- Occurs when the difference between the population and the sample is meaningful (statistically significant) and can be expected to continue for other samples drawn with the same (flawed) sampling strategy

### Bias

- Statistical Bias
	- Systematic measurement or sampling errors produced by the sampling process

### Random Selection

- Random Sampling
	- Proper definition of an accessible and representative population is not always easy to achieve
	- Definition of the population and specifying a sampling procedure are tasks that can be deceptively labor intensive
- Stratified Sampling
	- Population is divided into *strata* and random samples are taken from each stratum
	- Each strata is homogeneous to one another and are representative "microcosms" of the original population

### Size vs. Quality

- Despite being in the era of big data, smaller data pulls more than its fair share of weight
	- Time and effort spent on random sampling reduces bias and allows for greater attention to EDA
	- Data plotting, manual inspections, tracking down missing data and outliers may be difficult to do when there's hundreds of gigabytes to filter through
- Big Data
	- "When are massive amounts of data needed?"
	- Classic scenarios for big data is when the data is not only large but also very sparse ("empty" or filled with 0's)
	- Only when large quantities of data are accumulated, do the sparse data start to be used and analyzed effectively
	- Most modern machine learning models deal with dense (filled with nonzero data) datasets, not sparse

### Sample Mean vs. Population Mean

- Sample Mean
	- $\bar{x}$ represents the mean of a sample from a population
	- Information about samples is *observed*
- Population Mean
	- $\mu$ represents the mean of a population
	- Information about the population is *inferred* from smaller samples

## Selection Bias

- Selection Bias
	- Practice of selectively choosing data (consciously or unconsciously) in a way that leads to a conclusion that is misleading
	- Since repeated reviews of large datasets is a key portion of data science, selection bias is always lurking
		- One of the most prominent types of selection bias is the *vast search effect*, which states the running different models and "asking" different questions with a large dataset *will* result in something interesting
		- To guard against this, a holdout set is necessary to validate performance alongside *target shuffling*, which is a permutation test to test the validity of predictive associations generated by a data mining model
	- Other forms of selection bias include *nonrandom sampling*, *cherry picking* and ending an experiment because an "interesting" result appeared

### Regression to the Mean

- Regression to the Mean
	- Refers to an outcome involving successive measurements on a given variable
	- Falsely attaching focus and meaning to extreme outlier observations that are followed by central observations can lead to a form of selection bias
	- Note that regression to the mean is distinct from linear regression, in which a linear relationship is estimated between predictor variables and an outcome variable
- Ultimately, specifying a hypothesis and collecting data following proper randomization procedures and sampling ensures against bias

## Sampling Distribution of a Statistic

- Sampling Distribution
	- Refers to the distribution of some sample statistics over many samples drawn from the same population
	- Traditional statistics is concerned with making inferences from small samples to large populations
- Note that data distribution refers to the distribution of values within a single dataset, whereas sampling distribution refers to the distribution of a sample statistic (e.g. sample mean) across multiple samples taken from the same population

### Central Limit Theorem

- Central Limit Theorem (CLT)
	- Means drawn from multiple samples will represent the normal distribution, **even if the source population is not normally distributed**, assuming that the sample size ($n$) is large enough and the departure of data normality (normality assumption) is not too great
		- Samples must be independently and randomly drawn from the population
	- CLT is extremely important as it allows us to use the properties of the normal distribution to make inferences about the population, even if the population is not normal
		- Justifies the use of `t- tests` and `z- tests` as well as the confidence intervals and hypothesis tests, as long as $n$ is sufficiently large
	- $\bar{X} \sim N(\mu, \frac{\sigma}{\sqrt{n}})$
		- $\bar{X}$ represents the sample mean
		- $N(\mu, \frac{\sigma}{\sqrt{n}})$ represents a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$ 
		- $\mu$ represents population mean
		- $\sigma$ represents population standard deviation
	- Formal hypothesis tests and confidence intervals only play a small role in data science due to the existence of *bootstrap*, which diminishes CLT's importance in data science
		- Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or to perform inference without making strong assumptions about the underlying population distribution
		- Instead of relying on theoretical properties of the sampling distribution, bootstrap generates multiple resamples from the original sample data by sampling *with* replacement

### Standard Error

- Standard Error
	- Metric that sums up variability in the sample distribution 
	- Can be estimated using a statistic based on the standard deviation $s$ of the samples values and the sample size $n$
	- $Standard \: Error = SE = \frac{s}{\sqrt{n}}$ 
	- As sample size increases, standard error decreases, which is a relationship sometimes referred to as the *square root of $n$ rule* 
		- e.g. To reduce the standard error by a factor of 2, the sample size must be increased by a factor of 4
	- Collecting new samples to estimate the standard error is typically not feasible and computationally wasteful
		- In modern statistics, bootstrap resamples have become the standard way to estimate standard error since it can be used virtually anywhere and does not rely on distributional assumptions

## The Bootstrap

- Bootstrap
	- An effective way to estimate the sampling distribution of a statistic or of model parameters is to draw additional samples with replacement from the sample itself and recalculate the statistic/ model for each resample
	- Does not require assumptions about the data or the sample statistic being normally distributed
	- Conceptually, you replicate the original sample many times to the point where you have a hypothetical population comprised of samples
		- Drawing samples from this hypothetical population allows you to estimate a sampling distribution
		- Instead of replicating the sample many times, you can opt to simply sample with replacement, effectively creating an infinite population in which the probability of an element being drawn remains unchanged
		- Increasing the number of iterations of the bootstrap you do increases the accuracy of the estimate of the standard error
	- In cases of multivariate data, you can use classification and decision trees on top of bootstrap samples in order to get a better result than running a single tree, a technique known as *bagging*
	- Conceptually, bootstrap has been around since 1969
		- However, it was not considered a viable option due to limits in computational power at the time
	- Bootstrap does **not** compensate for small sample size, nor does it create new data or fill in holes in an existing dataset
		- Bootstrap's main purpose is to inform people about how additional samples will behave when drawn from a population mimicking the characteristics of the original sample

### Resampling vs. Bootstrapping

- Resampling
	- Although it is used interchangeably with bootstrap, resampling also includes permutation procedures, where multiple samples are combined and the sampling *may* be done without replacement
	- Bootstrap *always* implies sampling with replacement from an observed dataset

## Confidence Intervals

- Example
	- 90% Confidence Interval
		- Interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic
	- `x`% Confidence Interval
		- CI around a sample estimate should on average, contain similar sample estimates `x`% of the time
- Traditional vs. Modern
	- Traditional statistics reference confidence intervals as intervals generated by formulae such as the t- distribution (Student's t- distribution)
	- Most modern statistical tasks involving confidence intervals can be done using the bootstrap for most statistics or model parameters
- Level of Confidence
	- Percentage associated with the confidence interval
	- Increase in the level of confidence results in the increase (widening) of the interval
		- Same behavior applies when the size of the sample is decreased, as the uncertainty increases

## Normal Distribution

- Normal/ Gaussian Distribution
	- $f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$
	- 68% lies within one standard deviation of the mean
	- 95% lies within two standard deviations of the mean
- Note
	- Normal distribution's namesake does **not** come from the fact that most data follows its distribution 
	- Most forms of raw data are not normally distributed
	- Utility of normal distribution is derived from the fact that many statistics *are* normally distributed in their sampling distributions
	- Assumptions of normality are rarely taken and used as a last resort for when other assumptions do not apply

### Standard Normal & QQ Plots

- Standard Normal Distribution
	- Units on the x- axis are expressed in terms of standard deviations away from the mean
- Standardization
	- $z_i = \frac{{x_i - \bar{x}}}{{s}}$
		- $x_i$: individual data point
		- $\bar{x}$: mean
		- $s$: standard deviation
		- $z_i$: `z- score` (standardized value of $x_i$)
	- Transform each data point in a dataset such that it has a mean of 0 and a standard deviation of 1
- Normalization
	- $x'_i = \frac{{x_i - x_{\text{min}}}}{{x_{\text{max}} - x_{\text{min}}}}$
		- $x_i$: individual data point
		- $x_{\text{min}}$: minimum value in dataset
		- $x_{\text{max}}$: maximum value in dataset
		- $x'_i$: normalized value of $x_i$
	- Scale each data point in a dataset to a fixed range (e.g. 0 to 1)
- QQ Plot
	- Plot that visually determines how close a sample is to a specified distribution
	- Plots z- scores on the y- axis and the corresponding quantile for a value's rank on the x- axis
	- If the data is normalized (normal distribution), the data points correspond directly to the number of standard deviations away from the mean
	- Points close to the diagonal line can be considered close to normal
- Note
	- Converting data to z- scores (e.g. standardization or normalization tasks) does **not** make the data normally distributed, it only scales them to be used for comparison purposes

## Long- Tailed Distributions

- X

## t- Distribution

- X

## Binomial Distribution

- X

## Chi- Square Distribution

- X

## F- Distribution

- X

## Poisson & Related Distribution

### Poisson Distribution

- X

### Exponential Distribution

- X

### Estimating Failure Rate

- X

### Weibull Distribution

- X