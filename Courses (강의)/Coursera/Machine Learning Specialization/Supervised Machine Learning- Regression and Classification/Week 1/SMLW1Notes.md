# SML- Week 1: Introduction to Machine Learning

#machine_learning 

## Overview of Machine Learning

### Welcome to Machine Learning

- Basically, applications of machine learning are widespread

### Applications of Machine Learning

- Basically, applications of machine learning are widespread

## Supervised vs. Unsupervised Machine Learning

### What is Machine Learning?

- What is Machine Learning?
	- "Field of study that gives computers the ability to learn without being explicitly programmed" - Arthur Samuel (1959)
	- Generally speaking, the more opportunities you give for a learning algorithm to learn, the better it will perform
- Machine Learning Algorithms
	- Supervised Learning (Course 1 + Course 2)
		- Used most in real- world applications
		- Rapid advancement
	- Unsupervised Learning (Course 3)
	- Recommender Systems (Course 3)
	- Reinforcement Learning (Course 3)

### Supervised Learning (Part 1)

- Supervised Learning
	- Refers to algorithms that learn `x` to `y` or input to output mappings
	- Learning from being given "right" answers
- Regression: Housing Price Prediction
	- Regression
		- Predict a number from infinitely many possible outputs
	- Predicting housing prices based on the size of the house
	- Options
		- Line of Best Fit (Straight Line)
		- Curved Line
	- Labels
		- `x`: house size in square feet
		- `y`: price of the house in dollars

### Supervised Learning (Part 2)

- Classification: Breast Cancer Detection
	- Classification
		- Classification algorithms predict categories
		- Classification is different from regression in that we're trying to predict only a small number (finite) of possibilities (in this case two and later, three)
	- Figure out if a tumor is malignant or benign using a patient's medical records
	- Labels
		- `x`: size of the tumor in cm
		- `y`: status of the tumor (benign: `0`, malignant: `1- Type 1`, `1-Type 2`)
- Two or More Inputs
	- Using the previous example, now assume that you also have information regarding the patient's age
	- With the size of the tumor as the `x axis` and the age of the patient as the `y axis`, plot the status of the tumor & determine a *boundary line* that determines whether or not a tumor is benign or malignant
		- Can use things like KNN, decision trees, random forest, SVMs, Naive Bayes, logistic regression, neural networks from here

### Unsupervised Learning (Part 1)

- Unsupervised Learning
	- Find something interesting (e.g. pattern, structure) in *unlabeled* data
	- We do not *supervise* the algorithm to give a "right" answer for every input
- Clustering Algorithms
	- Finds clusters (structure) that lie within the data
	- Example
		- Google News
			- Groups related stories together by going through thousands of articles
		- DNA Microarray
			- Plot each individual on the x- axis and their genes on the y- axis
			- Color the individual cell by denoting whether or not that gene is active within the individual
			- Clustering algorithms can be run on the microarray to identify potential patterns that group people together based on the status of their genes
		- Customers
			- Grouping customers based on market segments to better serve customers

### Unsupervised Learning (Part 2)

- Unsupervised Learning
	- Data only comes with inputs `x`, but not output labels `y`
	- Algorithms has to find structure or patterns in the data
	- Examples
		- Clustering
			- Group similar data points together
		- Anomaly Detection
			- Find unusual data points
		- Dimensionality Reduction
			- Compress data using fewer numbers while losing as little data as possible

### Jupyter Notebooks

- [nbviewer](https://nbviewer.org/)

## Regression Model

### Linear Regression Model (Part 1)

- Example
	- Predicting the price of the house based on the size of the house
	- Use linear regression to create a line of best fit through the Portland housing data
		- Given a house that is 1250 square feet, calculate how much the house can be sold/ bought for based on the line of best fit
		- Regression models predict *numbers* and are considered as supervised learning models because it was fed with "right" answers
			- Note that regression has infinitely many possible outputs
		- Conversely, classification models predict *categories* and are also considered supervised learning as supervised learning models for the same reasons as above
			- Note that classification has a discrete, finite set of outputs
- Terminology
	- Training Set
		- Data used to train the model
	- `x`
		- Input/ feature variable
	- `y`
		- Output/ target variable
	- `m`
		- Number of training examples
	- `(x, y)`
		- Single training example
	- $(x^{(i)}, y^{(i)})$ 
		- $i^{th}$ training example

### Linear Regression Model (Part 2)

- $f$
	- Function generated by the learning algorithm
	- Also referred to as a hypothesis
	- Takes a new input $x$ and output an estimate, $\hat{y}$
- "How to represent $f$?"
	- Assuming $f$ is linear:
		- $f_{w, b}(x) = wx + b$
			- Linear regression with *one* variable
			- Univariate linear regression

### Cost Function Formula

- Training Set
	- Features
		- Size in $ft^2$, **x**
	- Targets
		- Price in \$1000's, **y**
- Model
	- $f_{w,b}(x) = wx + b$
	- $w, b$: parameters/ coefficients/ weights
		- Variables that will adjusted throughout the training process to improve the model's performance
- What does $w, b$ do?
	- $b$ is the y- intercept
	- $w$ is the slope
	- With linear regression, you want to select a line that will best fit the training data
		- In order to do so, you need to adjust the $b$ and $w$ such that the line is the best fit for the data
		- $\hat{y}^i = f_{w, b}(x^i)$
		- $f_{w, b} = wx^i + b$
- "How do you find $w, b$ such that $\hat{y}^i$ is close to $y^i$ for all $(x^i, y^i)$?"
	- Cost Function
		- Breakdown
			- $Error = (\hat{y} - y)^2$
				- Similar to $Residual = Observed - Predicted$ and $SSR = \sum Residual^2$
		- Overview (Squared Error Cost Function = MSE)
			- $J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}^i - y^i)^2$
			- MSE is commonly used for regression, especially linear regression

### Cost Function Intuition

- Assume you want to tune only one parameter, so $b$, the y- intercept is set to **0**
	- The parameter that will need to be optimized for this simple model is $w$, the slope
	- The goal then becomes: $minimize_{w} \: J(w)$ 
- Basically a similar example to the one listed in [SIGML- Chapter 5](../../../../../Books%20(ì±…)/In%20Progress/The%20StatQuest%20Illustrated%20Guide%20to%20Machine%20Learning/Chapter%205-%20Gradient%20Descent/SIGML-%20Chapter%205.md)
	- Talking about (a crude version of) parameter optimization, residuals, SSR and the MSE
- The goal of linear regression is to find the value of $w$ (and later, $b$) that minimizes $J$

### Visualizing the Cost Function

- Same example but with tuning both $w$ and $b$, instead of just $w$
	- The cost function becomes a 3- dimensional surface plane (bowl- shaped) when plotted instead of a 2- dimensional graph since we're dealing with 3 different axes ($w, b \: and \: J_{w, b}$)
- Note
	- Instead of using a 3- dimensional surface plot, you can opt to use a contour plot, which will reduce the visualization back down to 2- dimensions, since $J_{w, b}$ will be represented by the contours instead of an axis
		- The smallest ellipsis on the contour plot represents the minimum value of $J$

### Visualization Examples

- Comparing the performances of models with various $w$ and $b$
- Gradient Descent
	- Introduction of an algorithm that will automatically find the optimized values of $w$ and $b$

## Train the Model w/ Gradient Descent

### Gradient Descent

- Goal of Gradient Descent:
	- You have a function $J(w, b)$
	- You *want* $min_{w, b}J(w, b)$ 
- $J(w, b)$ is a loss function for linear regression, but the principles of gradient descent works for any function
	- Gradient descent works for functions that have more than two parameters as well, but more discussions on its scalability will be conducted later
- Gradient Descent Outline
	- Start with some arbitrary $w, b$
		- In linear regression, it won't matter too much what these initial values are
		- It's common to start with **0**
	- Keep changing $w, b$ to reduce $J(w, b)$, until we arrive near a minimum
		- **Note** that the loss function isn't always a parabola with a single minimum, so beware of functions that have many local minima
		- You want the **global** minimum
	- Determine a step size and go through the iteration again until you arrive at the local minimum

### Implementing Gradient Descent

- Gradient Descent Algorithm
	- $w_{new} = w_{current} - \alpha \frac{d}{dw}J(w, b)$
	- 

### Gradient Descent Intuition

- X

### Learning Rate

- X

### Gradient Descent for Linear Regression

- X

### Running Gradient Descent

- X
