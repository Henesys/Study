#cloud_computing 

# W10: Cloud Based Analytics

## Cloud Analytics

- Business Intelligence
	- Set of technologies and processes that use data to understand and analyze business performance
	- "Dark matter" of analytics, addresses necessary, but relatively simple questions that needs to be answered frequently
	- Presented in dashboards, simple data plots and reports
- Data Analytics
	- Science of examining data to draw conclusions
	- Subset of Business Intelligence
- Advanced Analytics
	- Uses more complex statistical techniques and machine learning to generate predictions & identify key performance indicators
	- Examples include fraud detection and recommender systems
- OLTP vs. OLAP
	- **Online Transaction Processing System (OLTP)**
		- Typically involves **most or all** of the columns in a row for a **small** amount of records
		- Using a database to *run* business
		- **RDBMS**
			- Structured Data
			- SQL
			- Each query returns small number of records
	- **Online Analytical Processing System (OLAP)**
		- Reads only a **few** columns for a **large** number of rows
		- Using a database to *understand* business
		- **Data Warehouse**
			- Structured Data
			- SQL
			- Each query covers many or all of the records
			- Typical query involves one column
	- Different access patterns between the two

## Data Cube

- What is a data cube?
	- OLAP data cube is a data structure that allows you to manipulate a multidimensional dataset into a two dimensional array
	- Method of representing data in your computer's memory
	- Nested array with compression schemes applied to it
- What is it used for?
	- Method of grabbing subsets of information in order to perform queries with them (under limitations with memory)
	- Allows you to cache subsets of data within the nested array
- How does it apply to clouds?
	- Data cubes refer to contexts in which data structures far outstrip the size of the hosting computer's main memory- which is where cloud gets involved
- Relationship between data cubes and SQL databases?
	- OLAP data cubes require teams to manage complicated pipelines to transform data from an SQL database into cubes
	- These ETL pipelines would need to be run prior to any analytical work
	- SQL databases and data warehouses had to be shaped in a way conducive to the data cube itself
- Who were Kimball and Inmon?
	- Early practitioners of data cube dimensional modeling and access patterns
- What were their contributions?
	- Developed repeatable methods to turn business reporting requirements into data warehouse designs, which allowed teams to extract the data they need into formats suitable for OLAP cubes
	- "Best Practices"
		- Kimball Dimensional Modeling
		- Inmon- Style Entity- Relationship Modeling
		- Data Vault Modeling
- What are common data cube operations?
	- Slicing
		- Produces a rectangular subset by selecting a single dimensional value
	- Dicing
		- Produces a sub- cube by selecting specific dimensional values
	- Drill Up/ Down
		- Levels of Data: Summarized (Up), Detailed (Down)
	- Roll Up
		- Summarizes data along a dimension
	- Pivot
		- Rotates the cube in space
- Relationship between OLAP cubes and row- oriented RDBMS?
	- Performance- wise, OLAP $>$ row- oriented RDBMS
		- This has become less important with recent technological advances in computers and columnar storage
- Strengths and weaknesses of each approach?
	- OLAP offers better analysis capabilities than RDBMS, which are **limited by SQL**
	- OLAP requires you to load a subset of the dimensions you're working on
	- Columnar databases can achieve similar performance in OLAP- type workloads onto the entire data without the need to extract and build new cubes

## Columnar Storage

- What are column stores?
	- Column- oriented systems
	- Each data element of a record is stored in a column
		- This allows a user to query just one data element (e.g. gym members who have paid for their membership), without having to read everything else associated with the record (e.g. name, age of person)
		- Uses a fraction of the memory and operations required for processing row- oriented blocks
		- A query that uses **5 columns** on a table that contains **100 columns** would only need to read 5% of the data
	- Good for analytical workloads (e.g. finding trends, computing average values)
		- **Read Optimized**
- What are some examples of column stores?
	- `MonetDB`
	- `VectorWise --> Ingres VectorWise --> Actian Vector`
	- `C- Store --> Vertica`
	- `SybaseIQ`
- What are some hardware optimizations for columnar storage?
	- Disk Access Pattern
		- All the read page are relevant column fields
	- CPU Caches
		- Reading multiple values on the same column in one run improves cache utilization & computational efficiency
	- SIMD
		- Vectorized instructions can be used to process multiple data points with a single CPU instruction
	- Compression
		- Lower information entropy results in higher compression
		- Choose the most effective compression method for each case
- What is column store file format?
	- Updates in columnar store is not recommended since you have to go to every column to update a "row"
	- Modern columnar databases limit the ability to update the data after it has been stored
	- Examples
		- Apache Parquet
			- Open source file format for Hadoop
				- Hive, Pig, Impala, Spark
			- Stores nested data structure in a flat columnar format
		- Apache ORC
			- Optimized Row Columnar Format
		- RCFile
		- Apache Kudu
		- ClickHouse

## Data Warehouse

- What motivates modern data warehouse architecture?
	- Cloud
		- Low cost storage
		- Improved scalability
		- Outsourcing of data warehousing management and security
		- Pay per use
	- Massively Parallel Processing (MPP)
		- Dividing computing operations to execute simultaneously across many separate computer processors (VM)
	- Columnar Storage
		- Cache
	- Vectorized Processing
		- SIMD
- What technologies enable it?
	- MariaDB with InfiniDB
	- PostgreSQL
	- Google BigQuery
		- Based on Google Dremel
	- AWS Redshift
		- Based on older version of PostgreSQL
		- Features that are suited for OLTP tasks have been removed for increased performance
- What are some examples of columnar- based data warehouse?
	- AWS Redshift
		- Columnar Store
		- Each block is 1 MB and is immutable
		- Clone blocks on write to avoid fragmentation
		- Small writes has similar cost to larger writes
	- Azure Synapse Analytics

## Data Lakes

- How does a data lake differ from a data warehouse?
	- Data warehouses cannot accommodate **unstructured** data
- Why use a data lake?
	- Stores massive amounts of raw data in its native form in a single location
	- Addresses need for a scalable, low- cost data repository that allows organizations to easily store all data types and analyze based on evidence based business decisions
	- Combines power of analytics with flexibility of big data models and limitless resources of the cloud
- What are the components of a data lake?
	- Object Storage (Location)
		- Azure Data Lake Storage
	- Move Data
		- AWS Data Pipeline
		- Azure Data Factory
	- Data Lake Schema Discovery
		- AWS Glue
		- Azure Data Catalog
	- SQL Exploration & Query
		- Apache Presto
		- AWS Athena
	- Lake Formation
		- AWS Lake Formation
		- Azure Data Share

## Other Cloud Analytics Services

- Serverless Analytics
	- Azure Analysis Services
	- AWS Redshift 
	- AWS Athena
	- AWS Glue
- Search- Based Analytics
	- Full Text Search
	- Search Analytics
	- ELK Stack
		- ElasticSearch
		- Logstash
		- Kibana
	- Search
		- AWS CloudSearch
		- Azure Cognitive Search
- Big Data Analytics
	- Azure HDInsight
		- Hadoop, Spark, Kafka, HBase, Storm
	- AWS Kinesis
	- Apache Kafka
- Graphical BI Tools
	- Tableau
	- AWS QuickSight
	- Azure Power BI

# W11: Graph Processing & Machine Learning

## Graph Databases

- What is a graph model?
	- Nodes (e.g. `Alice`, `A42`) with labels (e.g. `Person`, `Department`) in an entity have relationships (e.g. `:BELONGS_TO`) other entities
- What are relational databases?
	- Databases that are *joined* together by foreign keys
	- The *joins* are needed before any meaningful analysis can be done, which is why we need graph databases
	- Perform same operation on large numbers of data
	- Uses a relational model of data
	- Entity type has its own table
		- Rows are instances of the entity
		- Columns are representations of values associated with the instance
	- Rows in one table can be related to rows in another table via *unique key* per row
- What are graph databases?
	- Associate datasets that do not require *joins*
	- Structure of object- oriented applications
	- Database has an **explicit** graph structure
		- Each node knows its adjacent node
		- As number of nodes increases, cost of a local step remains constant
		- Small index lookup cost
	- Performance- wise, can be faster than relational databases for graph type queries 
		- e.g. "Who is a friend of a friend?"
	- Scales well
		- Less rigid schema permits easier evolution
- OLTP vs. OLAP Graph Databases
	- Transaction Processing (OLTP)
		- Query the graph in real- time
				- e.g. "What type of cereal does Alice buy?"
		- Real- time performance is only possible when a local traversal is enacted
		- Queries interact with a limited set of data
		- Examples
			- Neo4J
			- TinkerPop & Gremlin
			- Amazon Neptune
			- Azure CosmosDB
	- Analytical Processing (OLAP)
		- Process the entire graph
			- e.g. "What is the average price for cereal paid by people like Alice?"
		- Every vertex and edge is analyzed, possibly more than once
		- Results are not typically real- time
		- Bulk Synchronous Parallel (BSP) Model
			- Pregel
			- Giraph
			- Spark GraphX
			- GraphFrames
- Other types of graph databases?
	- Semantic Web
		- Resource Description Framework (RDF)
		- SPARQL Query Language
	- Labeled Property Graph
		- Data organized as nodes with relationships and properties

## Graph Processing 

- What is graph processing?
	- Way of retrieving information and analyzing information from a graph database
- What is a graph database?
	- Any storage system that provides index- free adjacency
		- Nodes represent entities
		- Properties are information that relate to nodes
		- Edges interconnect nodes to nodes or nodes to properties and represent relationships
- What is Pregel?
	- Cloud solution to distribute graph processing
	- Vertex- oriented computation model that allows you to define your own algorithm via a compute function
- How does it work?
	- Takes a graph and a corresponding set of vertex states as inputs
- What are its properties?
	- Message Passing
	- Guaranteed message delivery order
	- Messages delivered exactly once
	- Messages can be sent to any node
- How does it detect and respond to failures?
	- If the destination doesn't exist, the user's function is called
- What is Giraph?
	- Distributed graph processing framework designed to handle large- scale graph data and perform iterative computations across a cluster
	- Based on BSP
- How does it differ from Pregel?
	- Giraph is an open source implementation based on Pregel
	- Giraph is built on top of Hadoop
- Why was it created?
	- Addresses the need for scalable and efficient graph processing at Facebook (Pregel = Google)
- Differences between undirected and directed graphs?
	- Edges have direction in directed graph
	- Undirected graph have edges that are bidirectional in nature
- What is index- free adjacency?
	- Allows nodes to traverse between related entities
	- Relationships on a graph databases are stored as references/ pointers between nodes, allowing the database to navigate quickly
- Difference between graph databases and relational databases?
	- Joins are needed on relational databases
	- Joins are not needed on graph databases
- Limitations of MapReduce with respect to graph processing?
	- MapReduce is inefficient because the graph state must be stored at each stage of the graph algorithm
	- Each computational stage will produce a lot of communication between stages
- Limitations of current shared- memory systems for graph processing?
	- No fault tolerance
- Describe vertex oriented graph processing?
	- Based on BSP
	- Provides direct graph to Pregel
	- Runs computation at each  vertex, repeating until every computation at each vertex vortex to halt
	- Pregel returns directed graph
- Primitives in Pregel?
	- Vertices
		- First Class
	- Edges
		- Not First Class (Incidental)
- Are edges or are vertices first- class citizens in Pregel?
	- Vertices
- What are supersteps?
	- Iterations of Pregel
- What does each super step involve doing?
	- At each superstep (iteration), each vertex sends a message to its neighboring vertices, processes messages received in a previous superstep and **updates its state**
		- Get/ set vertex value
		- Get/ set outgoing edges values
		- Send/ receive messages
- Layout of a standard Pregel program?
	- Reads message from previous superstep
	- Sends message to nearby vertices
	- Updates state/ modifies vertex
- Describe the Pregel system architecture
	- Master/ Worker Model
		- Master
			- Maintains worker
			- Recovers faults of workers
			- Provides web- UI monitoring tool of job progress
		- Worker
			- Processes its task
			- Communicates with other workers
- Storage of temporary and persistent data in Pregel?
	- Persistent
		- Stored as files on a distributed storage system
			- e.g. HDFS, BigTable, GFS
	- Temporary
		- Stored on local disk
- Execution of a Pregel program?
	- Copies of a Pregel program execute on a cluster of machines
	- Master assigns partition of input to each worker
	- Each work loads vertices and marks them active
	- Superstep
		- Worker loops through active vertices and computes for each vertex
		- Message are sent asynchronous but before each superstep
		- Repeat until vertices become inactive
	- When computation halts, master instructs workers to save its portion of the graph
- How does Pregel guarantee fault tolerance?
	- Checkpointing
	- Failure Detection 
	- Recovery
- Describe checkpointing
	- Master periodically instructs workers to save the state of their partitions to persistent storage
- Describe failure detection
	- Using pings to check the status
- Describe recovery
	- Master reassigns graph partitions to the currently available workers
	- Workers all reload their partition state from the most recent checkpoint (rollback)
- What is Apache Giraph?
	- Open source implementation based on Pregel
- Describe the responsibilities of the following components in Giraph
	- ZooKeeper
		- Responsible for **computational state**
		- Partition/ worker mapping
		- Checkpoints paths, aggregator values, statistics
	- Master
		- Responsible for **coordination**
		- Assigns partitions to workers
		- Coordinates synchronization
		- Requests checkpoints
		- Aggregates aggregator values
		- Collects health statuses
	- Worker
		- Responsible for **vertices**
		- Invokes `compute()`
		- Sends, receives and assigns messages
		- Computes *local* aggregation values
- Describe the standard execution flow in Giraph
	- Overview
		- `Setup --> Compute <--> Synchronize --> Teardown`
	- Setup
		- Load graph from disk
		- Assigns vertices to workers
		- Check workers' health
	- Compute
		- Assign messages to workers
		- Iterate on active vertices
		- Invoke `compute()` on vertices
	- Synchronize
		- Send message to workers
		- Compute aggregators
		- Checkpoint
	- Teardown
		- Write back result
		- Write back aggregators
- Describe an algorithm to find connected components in Giraph
	- Propagate smallest vertex label to nearby vertices until convergence, which results in all vertices of a component having the same label
- What is GraphX?
	- Directed graph that contains both edges and vertices
	- Has properties attached to each vertex and edge
- What technologies does GraphX use?
	- Apache Spark & RDD (Resilient Distributed Dataset) to efficiently perform graph computations in parallel across a cluster
- Building a graph in GraphX?
	- Provides APIs to create graphs from RDDs of vertex and edge tuples
	- Define RDDs for vertices and edges
		- Use GraphX APIs to combine  into Graph object
- How does GraphX compare to Giraph/ Pregel?
	- All of them are frameworks for distributed graph processing
	- Built on top of Apache Spark, which is a more general purpose distributed computing platform
		- Giraph/ Pregel are specialized for graph processing
	- Giraph/ Pregel are more focused on iterative graph processing algorithms are used when low- latency processing is required
- Graph operators supported by GraphX?
	- `vertices, edges`
		- Retrieves vertices, edges of graph
	- `mapVertices, mapEdges`
		- Applies a function to each vertices, edges in the graph
	- `aggregateMessages`
		- Aggregates messages from neighboring vertices
	- `joinVertices`
		- Joins vertex attributes with another RDD
	- `subgraph`
		- Create subgraph
	- `connectedComponents, triangleCount, pagerank, shortestPaths`
		- Pre- implemented graph algorithms 

## Machine Learning in the Cloud

- How does data mining and machine learning relate to artificial intelligence?
	- Subset of AI
		- Analyze, mine and summarize large datasets
		- Extract knowledge from past data
		- Predict trends in future data
- What are some applications?
	- Information Retrieval
	- Statistics
	- Linear Algebra
	- Business
- What is the AI/ ML Life Cycle Workflow?
	- Gathering Data
	- Data Preparation
	- Data Wrangling
	- Analyze Data
	- Train Model
	- Test Model
	- Deployment
- What is the OSEMN Data Science Model?
	- Obtain
		- Obtain data from source in the cloud
		- Examples
			- AES Open Data Registry (AWS)
			- Azure Open Datasets
			- Google Cloud Public Datasets
			- Cloud Storage
			- Cloud Databases
	- Scrub
		- Clean the collected data, wrangle it and preprocess it for analysis
	- Explore
		- Explore and analyze data to gain insights (EDA)
		- Examples
			- Spark
			- AWS EMR
	- Model
		- Use statistical and machine learning models built on the preprocessed data to make predictions, classify data or discover patterns
		- Feature Engineering
		- Examples
			- Spark MLLib
			- Google Cloud Dataproc
			- Mahout
	- Interpret
		- Results from models are interpreted and communicated
		- Examples
			- Tableau
			- D3.js
			- `Seaborn`
- What is a hyperparameter?
	- Parameters that train the model and are not learned from the data
- What is AutoML?
	- Automating tasks such as preprocessing, model selection, feature engineering and hyperparameter tuning
- How do they relate to one another?
	- Hyperparameters influence the behavior and performance of models
	- AutoML automates and includes hyperparameter optimization as one of its features
- What is the Google Cloud AI Platform?
	- Cloud platform by Google for building, training and deploying machine learning models at scale, designed to streamline the machine learning workflow
- What tools does it provide?
	- AI Platform Notebooks
		- Managed notebooks
	- AI Platform Training
		- Training with hyperparameter optimizations
	- Continuous Evaluation
		- Model optimization
	- AI Platform Predictions
		- Server model hosting deployment
	- Kubeflow
		- Deployment of machine learning workflows on Kubernetes
	- AutoML Tables
- What does Microsoft Azure offer for cloud- based machine learning?
	- Microsoft Azure Machine Learning
- Describe how AWS SageMaker supports ML on the cloud
	- SageMaker simplifies the machine learning workflow and enables scalable/ cost effective model deployment
- What is human in the loop AI?
	- Selective inclusion of human participation in ML
- Why use it?
	- Harness the efficiency of computers while utilizing human intelligence through interaction and curation
	- Allows you to reframe an automation problem into a HCI problem
- Strengths and weaknesses?
	- Strengths
		- Transparency
		- Human Judgement
		- No need to build a "perfect" AI system
	- Weaknesses
		- Cost
		- Slower Processing
		- Human Judgement (Bias)
- What are tools that support it?
	- AWS SageMaker Ground Truth
	- AWS SageMaker Augmented AI
	- AWS Mechanical Turk
- What are examples of unstructured data in the context of cloud machine learning?
	- Vision
	- Voice
	- Language
- What are ML cloud tools specifically designed to handle unstructured data?
	- Vision
		- Google Cloud Vision API
	- Voice
		- Amazon Rekognition
	- Language
		- AWS Lex
- What is Mahout?
	- Open source library for scalable machine learning algorithms
	- Built on VM
- What are its goals?
	- Provide scalable machine learning algorithms that can efficiently handle large datasets distributed across a cluster
	- Enables recommendation, clustering, classification and collaborative filtering using distributed computing frameworks
- What is Spark?
	- Open source distributed computing framework that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance
- What are its goals?
	- Providing unified/ combined framework for big data processing that is efficient and fault- tolerant
	- Highlights include batch processing, real- time streaming, machine learning and graph processing
- What is collaborative filtering?
	- Making predictions about the interests of a user by collecting preferences from other users (collaboration)
	- Analyzes interactions between users and items (e.g. reviews, purchase history) to identify similarities
- What is clustering?
	- Unsupervised machine learning technique that groups similar data points together based on their features
	- Partitions dataset into clusters where the points within the same cluster are similar to each other than those in other clusters
- What is it used for?
	- Discover underlying patterns or structures within data
	- Identifying outliers
	- Segmenting users based on their behavior
	- Grouping points based on their features
- What is the K- Means algorithm used for?
	- Clusters data points into predefined number of clusters
	- Minimizes distance between data points and the centroid of their assigned cluster while maximizing the distance between centroids of different clusters
- How does it work?
	- Iteratively assigns data points to the nearest centroid
	- Updates centroids based on mean of the data points assigned to each cluster
	- Continues until convergence, where centroids no longer change significantly **or** max iterations have been reached
- Optimizations for K- Means?
	- Random Initialization
	- Parallelization 
	- K- Means++ Initialization
- What is classification with respect to machine learning?
	- Supervised machine learning task that categorizes input data into predefined classes or categories
	- Maps input features to output labels, allowing the model to predict the class of unseen data instances
- Describe a few use cases of classification
	- Spam Filtering
	- Sentiment Analysis
- Describe a few models which can be used for classification
	- Logistic Regression
	- Decision Trees
	- Random Forest
	- K- Nearest Neighbors (KNN)
	- Naive Bayes
- Describe the Naive Bayes algorithm
	- Multiclass classification algorithm with "naive" assumption of independence between every pair of features
	- Calculates probability of each class given input features and selects the class with the highest probability as the predicted class
- What is Frequent Pattern Mining (FPM)?
	- Data mining technique used to discover frequently occurring patterns in a dataset
	- Identifies items that often co- occur together in events
- What are its use cases?
	- Recommendation Systems
- Describe the Apriori algorithm
	- Classic algorithm for FPM
	- Iteratively discovers frequent item sets by generating candidate item sets and pruning those that do not meet a minimum support threshold
		- Frequent item sets are extended larger and larger item sets as long as they appear sufficiently often in the database
	- Reduces search space using the Apriori property that any subset of a frequent item set must be frequent

# W12: Streaming Systems

## Streaming Systems

- Why do we need real- time stream processing systems?
- What are representative cloud streaming engines?
- What are spouts, bolts, streams and topologies in Apache Storm?
- What are the different grouping ways in Storm?
- What are the three flavors guaranteeing message processing?
- Where are they used?
- How does Trident interact with exactly once processing?
- What is the structure of a Storm cluster?
- How does Storm guarantee fault tolerance?
- How is Thrift used in Storm?
- What is the `IScheduler` and multi- tenant scheduler in Storm?
- What is stateful stream processing?
- What is the difference between Storm streaming and Spark streaming?
- What are the advantages and disadvantages of microbatch in Spark streaming?
- What applications are more suitable for Spark streaming?
- What is Lambda architecture in stream processing?
- What is Kappa architecture in stream processing?
- Why do we need Lambda and Kappa architecture?
- What are the general steps in a streaming ecosystem?

# W13: Virtualization & Containers

## Virtualization

- What is virtualization?
- What is its main idea?
- User & Kernel Modes
- CPU Privilege Levels
- What are differences between different types of virtualizations?
- What is the definition of each type?
- What are some examples of each type?
- Xen
- Binary Translations
- What is hardware virtualization?
- What is its history?

## Containers

- What is operating system- level virtualization?
- What are examples of OS virtualization?
- What is OS Virtualization?
- What are OS Containers?
- Differences between VM and containers?
- What are the three building blocks of containers?

## Docker

- What is the Union File System?
- What are Docker images?
- Describe Docker architecture and its components
- What are container network models?

# W14: Container Orchestration & Docker Swarm

## Docker 

- What is Dockerfile?
- How do you use it?
- What is Docker Swarm?
- What are some examples of Swarm Services?
- What are states of Swarm Services?
- What are the different types of Docker networks?
- What are the its usages?
- What is service discovery in Docker Swarm?
- What are the three ways to map hosts to containers?
- What is Docker Compose?
- What is its usage?

# W15: Container Orchestration & Kubernetes

## Kubernetes

- What is Kubernetes?
- What is the function of Kubernetes?
- What are the primary advantages of using Kubernetes?
- What is the architecture of Kubernetes?
- What are the components of master nodes and worker nodes?
- What are three pod container design patterns?
- Describe the building blocks of Kubernetes
	- Nodes
	- Pods
	- Labels
	- Selectors
	- Controllers
- What is the Kubernetes Service?
- What does it do?
- What is the Kubernetes Service Proxy?
- Differences between Kubernetes and Docker Swarm?

# W16: Future Developments in the Cloud

## Thirteen Predictions

- Adoption of cloud computing will grow rapidly
- Cloud will become more global
- Regulated industries will move to the cloud
- Storage capacity will continue to increase rapidly
- SSD's share of the market will grow, but there will still be a lot of HDD and magnetic tape in use
- Cloud will continue to support AI
- There is a symbiotic/ mutually- beneficial relationship between AI and clouds
- Data center accelerator's market growth will continue to skyrocket
- Future of cloud computing access is mobile
- FaaS will continue to grow in popularity and use
- Low- code/ No code/ citizen development will become increasingly important to the industry as it is a way to help deal with the predicted 1 million software developer shortfall in the US
- Increased adoption of clouds means increased risks of security breaches
- IoT will grow and fuel the growth of the cloud industry
- Hybrid clouds/ multi- clouds/ omni- clouds will become more feasible and widely- used
- Demand for cloud professionals will grow