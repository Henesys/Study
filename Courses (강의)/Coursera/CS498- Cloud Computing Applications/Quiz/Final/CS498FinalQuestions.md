#cloud_computing 

# W10: Cloud Based Analytics

## Cloud Analytics

- Business Intelligence
	- Set of technologies and processes that use data to understand and analyze business performance
	- "Dark matter" of analytics, addresses necessary, but relatively simple questions that needs to be answered frequently
	- Presented in dashboards, simple data plots and reports
- Data Analytics
	- Science of examining data to draw conclusions
	- Subset of Business Intelligence
- Advanced Analytics
	- Uses more complex statistical techniques and machine learning to generate predictions & identify key performance indicators
	- Examples include fraud detection and recommender systems
- OLTP vs. OLAP
	- **Online Transaction Processing System (OLTP)**
		- Typically involves **most or all** of the columns in a row for a **small** amount of records
		- Using a database to *run* business
		- **RDBMS**
			- Structured Data
			- SQL
			- Each query returns small number of records
	- **Online Analytical Processing System (OLAP)**
		- Reads only a **few** columns for a **large** number of rows
		- Using a database to *understand* business
		- **Data Warehouse**
			- Structured Data
			- SQL
			- Each query covers many or all of the records
			- Typical query involves one column
	- Different access patterns between the two

## Data Cube

- What is a data cube?
	- OLAP data cube is a data structure that allows you to manipulate a multidimensional dataset into a two dimensional array
	- Method of representing data in your computer's memory
	- Nested array with compression schemes applied to it
- What is it used for?
	- Method of grabbing subsets of information in order to perform queries with them (under limitations with memory)
	- Allows you to cache subsets of data within the nested array
- How does it apply to clouds?
	- Data cubes refer to contexts in which data structures far outstrip the size of the hosting computer's main memory- which is where cloud gets involved
- Relationship between data cubes and SQL databases?
	- OLAP data cubes require teams to manage complicated pipelines to transform data from an SQL database into cubes
	- These ETL pipelines would need to be run prior to any analytical work
	- SQL databases and data warehouses had to be shaped in a way conducive to the data cube itself
- Who were Kimball and Inmon?
	- Early practitioners of data cube dimensional modeling and access patterns
- What were their contributions?
	- Developed repeatable methods to turn business reporting requirements into data warehouse designs, which allowed teams to extract the data they need into formats suitable for OLAP cubes
	- "Best Practices"
		- Kimball Dimensional Modeling
		- Inmon- Style Entity- Relationship Modeling
		- Data Vault Modeling
- What are common data cube operations?
	- Slicing
		- Produces a rectangular subset by selecting a single dimensional value
	- Dicing
		- Produces a sub- cube by selecting specific dimensional values
	- Drill Up/ Down
		- Levels of Data: Summarized (Up), Detailed (Down)
	- Roll Up
		- Summarizes data along a dimension
	- Pivot
		- Rotates the cube in space
- Relationship between OLAP cubes and row- oriented RDBMS?
	- Performance- wise, OLAP $>$ row- oriented RDBMS
		- This has become less important with recent technological advances in computers and columnar storage
- Strengths and weaknesses of each approach?
	- OLAP offers better analysis capabilities than RDBMS, which are **limited by SQL**
	- OLAP requires you to load a subset of the dimensions you're working on
	- Columnar databases can achieve similar performance in OLAP- type workloads onto the entire data without the need to extract and build new cubes

## Columnar Storage

- What are column stores?
	- Column- oriented systems
	- Each data element of a record is stored in a column
		- This allows a user to query just one data element (e.g. gym members who have paid for their membership), without having to read everything else associated with the record (e.g. name, age of person)
		- Uses a fraction of the memory and operations required for processing row- oriented blocks
		- A query that uses **5 columns** on a table that contains **100 columns** would only need to read 5% of the data
	- Good for analytical workloads (e.g. finding trends, computing average values)
		- **Read Optimized**
- What are some examples of column stores?
	- `MonetDB`
	- `VectorWise --> Ingres VectorWise --> Actian Vector`
	- `C- Store --> Vertica`
	- `SybaseIQ`
- What are some hardware optimizations for columnar storage?
	- Disk Access Pattern
		- All the read page are relevant column fields
	- CPU Caches
		- Reading multiple values on the same column in one run improves cache utilization & computational efficiency
	- SIMD
		- Vectorized instructions can be used to process multiple data points with a single CPU instruction
	- Compression
		- Lower information entropy results in higher compression
		- Choose the most effective compression method for each case
- What is column store file format?
	- Updates in columnar store is not recommended since you have to go to every column to update a "row"
	- Modern columnar databases limit the ability to update the data after it has been stored
	- Examples
		- Apache Parquet
			- Open source file format for Hadoop
				- Hive, Pig, Impala, Spark
			- Stores nested data structure in a flat columnar format
		- Apache ORC
			- Optimized Row Columnar Format
		- RCFile
		- Apache Kudu
		- ClickHouse

## Data Warehouse

- What motivates modern data warehouse architecture?
	- Cloud
		- Low cost storage
		- Improved scalability
		- Outsourcing of data warehousing management and security
		- Pay per use
	- Massively Parallel Processing (MPP)
		- Dividing computing operations to execute simultaneously across many separate computer processors (VM)
	- Columnar Storage
		- Cache
	- Vectorized Processing
		- SIMD
- What technologies enable it?
	- MariaDB with InfiniDB
	- PostgreSQL
	- Google BigQuery
		- Based on Google Dremel
	- AWS Redshift
		- Based on older version of PostgreSQL
		- Features that are suited for OLTP tasks have been removed for increased performance
- What are some examples of columnar- based data warehouse?
	- AWS Redshift
		- Columnar Store
		- Each block is 1 MB and is immutable
		- Clone blocks on write to avoid fragmentation
		- Small writes has similar cost to larger writes
	- Azure Synapse Analytics

## Data Lakes

- How does a data lake differ from a data warehouse?
	- Data warehouses cannot accommodate **unstructured** data
- Why use a data lake?
	- Stores massive amounts of raw data in its native form in a single location
	- Addresses need for a scalable, low- cost data repository that allows organizations to easily store all data types and analyze based on evidence based business decisions
	- Combines power of analytics with flexibility of big data models and limitless resources of the cloud
- What are the components of a data lake?
	- Object Storage (Location)
		- Azure Data Lake Storage
	- Move Data
		- AWS Data Pipeline
		- Azure Data Factory
	- Data Lake Schema Discovery
		- AWS Glue
		- Azure Data Catalog
	- SQL Exploration & Query
		- Apache Presto
		- AWS Athena
	- Lake Formation
		- AWS Lake Formation
		- Azure Data Share

## Other Cloud Analytics Services

- Serverless Analytics
	- Azure Analysis Services
	- AWS Redshift 
	- AWS Athena
	- AWS Glue
- Search- Based Analytics
	- Full Text Search
	- Search Analytics
	- ELK Stack
		- ElasticSearch
		- Logstash
		- Kibana
	- Search
		- AWS CloudSearch
		- Azure Cognitive Search
- Big Data Analytics
	- Azure HDInsight
		- Hadoop, Spark, Kafka, HBase, Storm
	- AWS Kinesis
	- Apache Kafka
- Graphical BI Tools
	- Tableau
	- AWS QuickSight
	- Azure Power BI

# W11: Graph Processing & Machine Learning

## Graph Databases

- What is a graph model?
	- Nodes (e.g. `Alice`, `A42`) with labels (e.g. `Person`, `Department`) in an entity have relationships (e.g. `:BELONGS_TO`) other entities
- What are relational databases?
	- Databases that are *joined* together by foreign keys
	- The *joins* are needed before any meaningful analysis can be done, which is why we need graph databases
	- Perform same operation on large numbers of data
	- Uses a relational model of data
	- Entity type has its own table
		- Rows are instances of the entity
		- Columns are representations of values associated with the instance
	- Rows in one table can be related to rows in another table via *unique key* per row
- What are graph databases?
	- Associate datasets that do not require *joins*
	- Structure of object- oriented applications
	- Database has an **explicit** graph structure
		- Each node knows its adjacent node
		- As number of nodes increases, cost of a local step remains constant
		- Small index lookup cost
	- Performance- wise, can be faster than relational databases for graph type queries 
		- e.g. "Who is a friend of a friend?"
	- Scales well
		- Less rigid schema permits easier evolution
- OLTP vs. OLAP Graph Databases
	- Transaction Processing (OLTP)
		- Query the graph in real- time
				- e.g. "What type of cereal does Alice buy?"
		- Real- time performance is only possible when a local traversal is enacted
		- Queries interact with a limited set of data
		- Examples
			- Neo4J
			- TinkerPop & Gremlin
			- Amazon Neptune
			- Azure CosmosDB
	- Analytical Processing (OLAP)
		- Process the entire graph
			- e.g. "What is the average price for cereal paid by people like Alice?"
		- Every vertex and edge is analyzed, possibly more than once
		- Results are not typically real- time
		- Bulk Synchronous Parallel (BSP) Model
			- Pregel
			- Giraph
			- Spark GraphX
			- GraphFrames
- Other types of graph databases?
	- Semantic Web
		- Resource Description Framework (RDF)
		- SPARQL Query Language
	- Labeled Property Graph
		- Data organized as nodes with relationships and properties

## Graph Processing 

- What is graph processing?
	- Way of retrieving information and analyzing information from a graph database
- What is a graph database?
	- Any storage system that provides index- free adjacency
		- Nodes represent entities
		- Properties are information that relate to nodes
		- Edges interconnect nodes to nodes or nodes to properties and represent relationships
- What is Pregel?
	- Cloud solution to distribute graph processing
	- Vertex- oriented computation model that allows you to define your own algorithm via a compute function
- How does it work?
	- Takes a graph and a corresponding set of vertex states as inputs
- What are its properties?
	- Message Passing
	- Guaranteed message delivery order
	- Messages delivered exactly once
	- Messages can be sent to any node
- How does it detect and respond to failures?
	- If the destination doesn't exist, the user's function is called
- What is Giraph?
	- Distributed graph processing framework designed to handle large- scale graph data and perform iterative computations across a cluster
	- Based on BSP
- How does it differ from Pregel?
	- Giraph is an open source implementation based on Pregel
	- Giraph is built on top of Hadoop
- Why was it created?
	- Addresses the need for scalable and efficient graph processing at Facebook (Pregel = Google)
- Differences between undirected and directed graphs?
	- Edges have direction in directed graph
	- Undirected graph have edges that are bidirectional in nature
- What is index- free adjacency?
	- Allows nodes to traverse between related entities
	- Relationships on a graph databases are stored as references/ pointers between nodes, allowing the database to navigate quickly
- Difference between graph databases and relational databases?
	- Joins are needed on relational databases
	- Joins are not needed on graph databases
- Limitations of MapReduce with respect to graph processing?
	- MapReduce is inefficient because the graph state must be stored at each stage of the graph algorithm
	- Each computational stage will produce a lot of communication between stages
- Limitations of current shared- memory systems for graph processing?
	- No fault tolerance
- Describe vertex oriented graph processing?
	- Based on BSP
	- Provides direct graph to Pregel
	- Runs computation at each  vertex, repeating until every computation at each vertex vortex to halt
	- Pregel returns directed graph
- Primitives in Pregel?
	- Vertices
		- First Class
	- Edges
		- Not First Class (Incidental)
- Are edges or are vertices first- class citizens in Pregel?
	- Vertices
- What are supersteps?
	- Iterations of Pregel
- What does each super step involve doing?
	- At each superstep (iteration), each vertex sends a message to its neighboring vertices, processes messages received in a previous superstep and **updates its state**
		- Get/ set vertex value
		- Get/ set outgoing edges values
		- Send/ receive messages
- Layout of a standard Pregel program?
	- Reads message from previous superstep
	- Sends message to nearby vertices
	- Updates state/ modifies vertex
- Describe the Pregel system architecture
	- Master/ Worker Model
		- Master
			- Maintains worker
			- Recovers faults of workers
			- Provides web- UI monitoring tool of job progress
		- Worker
			- Processes its task
			- Communicates with other workers
- Storage of temporary and persistent data in Pregel?
	- Persistent
		- Stored as files on a distributed storage system
			- e.g. HDFS, BigTable, GFS
	- Temporary
		- Stored on local disk
- Execution of a Pregel program?
	- Copies of a Pregel program execute on a cluster of machines
	- Master assigns partition of input to each worker
	- Each work loads vertices and marks them active
	- Superstep
		- Worker loops through active vertices and computes for each vertex
		- Message are sent asynchronous but before each superstep
		- Repeat until vertices become inactive
	- When computation halts, master instructs workers to save its portion of the graph
- How does Pregel guarantee fault tolerance?
	- Checkpointing
	- Failure Detection 
	- Recovery
- Describe checkpointing
	- Master periodically instructs workers to save the state of their partitions to persistent storage
- Describe failure detection
	- Using pings to check the status
- Describe recovery
	- Master reassigns graph partitions to the currently available workers
	- Workers all reload their partition state from the most recent checkpoint (rollback)
- What is Apache Giraph?
	- Open source implementation based on Pregel
- Describe the responsibilities of the following components in Giraph
	- ZooKeeper
		- Responsible for **computational state**
		- Partition/ worker mapping
		- Checkpoints paths, aggregator values, statistics
	- Master
		- Responsible for **coordination**
		- Assigns partitions to workers
		- Coordinates synchronization
		- Requests checkpoints
		- Aggregates aggregator values
		- Collects health statuses
	- Worker
		- Responsible for **vertices**
		- Invokes `compute()`
		- Sends, receives and assigns messages
		- Computes *local* aggregation values
- Describe the standard execution flow in Giraph
	- Overview
		- `Setup --> Compute <--> Synchronize --> Teardown`
	- Setup
		- Load graph from disk
		- Assigns vertices to workers
		- Check workers' health
	- Compute
		- Assign messages to workers
		- Iterate on active vertices
		- Invoke `compute()` on vertices
	- Synchronize
		- Send message to workers
		- Compute aggregators
		- Checkpoint
	- Teardown
		- Write back result
		- Write back aggregators
- Describe an algorithm to find connected components in Giraph
	- Propagate smallest vertex label to nearby vertices until convergence, which results in all vertices of a component having the same label
- What is GraphX?
	- Directed graph that contains both edges and vertices
	- Has properties attached to each vertex and edge
- What technologies does GraphX use?
	- Apache Spark & RDD (Resilient Distributed Dataset) to efficiently perform graph computations in parallel across a cluster
- Building a graph in GraphX?
	- Provides APIs to create graphs from RDDs of vertex and edge tuples
	- Define RDDs for vertices and edges
		- Use GraphX APIs to combine  into Graph object
- How does GraphX compare to Giraph/ Pregel?
	- All of them are frameworks for distributed graph processing
	- Built on top of Apache Spark, which is a more general purpose distributed computing platform
		- Giraph/ Pregel are specialized for graph processing
	- Giraph/ Pregel are more focused on iterative graph processing algorithms are used when low- latency processing is required
- Graph operators supported by GraphX?
	- `vertices, edges`
		- Retrieves vertices, edges of graph
	- `mapVertices, mapEdges`
		- Applies a function to each vertices, edges in the graph
	- `aggregateMessages`
		- Aggregates messages from neighboring vertices
	- `joinVertices`
		- Joins vertex attributes with another RDD
	- `subgraph`
		- Create subgraph
	- `connectedComponents, triangleCount, pagerank, shortestPaths`
		- Pre- implemented graph algorithms 

## Machine Learning in the Cloud

- How does data mining and machine learning relate to artificial intelligence?
	- Subset of AI
		- Analyze, mine and summarize large datasets
		- Extract knowledge from past data
		- Predict trends in future data
- What are some applications?
	- Information Retrieval
	- Statistics
	- Linear Algebra
	- Business
- What is the AI/ ML Life Cycle Workflow?
	- Gathering Data
	- Data Preparation
	- Data Wrangling
	- Analyze Data
	- Train Model
	- Test Model
	- Deployment
- What is the OSEMN Data Science Model?
	- Obtain
		- Obtain data from source in the cloud
		- Examples
			- AES Open Data Registry (AWS)
			- Azure Open Datasets
			- Google Cloud Public Datasets
			- Cloud Storage
			- Cloud Databases
	- Scrub
		- Clean the collected data, wrangle it and preprocess it for analysis
	- Explore
		- Explore and analyze data to gain insights (EDA)
		- Examples
			- Spark
			- AWS EMR
	- Model
		- Use statistical and machine learning models built on the preprocessed data to make predictions, classify data or discover patterns
		- Feature Engineering
		- Examples
			- Spark MLLib
			- Google Cloud Dataproc
			- Mahout
	- Interpret
		- Results from models are interpreted and communicated
		- Examples
			- Tableau
			- D3.js
			- `Seaborn`
- What is a hyperparameter?
	- Parameters that train the model and are not learned from the data
- What is AutoML?
	- Automating tasks such as preprocessing, model selection, feature engineering and hyperparameter tuning
- How do they relate to one another?
	- Hyperparameters influence the behavior and performance of models
	- AutoML automates and includes hyperparameter optimization as one of its features
- What is the Google Cloud AI Platform?
	- Cloud platform by Google for building, training and deploying machine learning models at scale, designed to streamline the machine learning workflow
- What tools does it provide?
	- AI Platform Notebooks
		- Managed notebooks
	- AI Platform Training
		- Training with hyperparameter optimizations
	- Continuous Evaluation
		- Model optimization
	- AI Platform Predictions
		- Server model hosting deployment
	- Kubeflow
		- Deployment of machine learning workflows on Kubernetes
	- AutoML Tables
- What does Microsoft Azure offer for cloud- based machine learning?
	- Microsoft Azure Machine Learning
- Describe how AWS SageMaker supports ML on the cloud
	- SageMaker simplifies the machine learning workflow and enables scalable/ cost effective model deployment
- What is human in the loop AI?
	- Selective inclusion of human participation in ML
- Why use it?
	- Harness the efficiency of computers while utilizing human intelligence through interaction and curation
	- Allows you to reframe an automation problem into a HCI problem
- Strengths and weaknesses?
	- Strengths
		- Transparency
		- Human Judgement
		- No need to build a "perfect" AI system
	- Weaknesses
		- Cost
		- Slower Processing
		- Human Judgement (Bias)
- What are tools that support it?
	- AWS SageMaker Ground Truth
	- AWS SageMaker Augmented AI
	- AWS Mechanical Turk
- What are examples of unstructured data in the context of cloud machine learning?
	- Vision
	- Voice
	- Language
- What are ML cloud tools specifically designed to handle unstructured data?
	- Vision
		- Google Cloud Vision API
	- Voice
		- Amazon Rekognition
	- Language
		- AWS Lex
- What is Mahout?
	- Open source library for scalable machine learning algorithms
	- Built on VM
- What are its goals?
	- Provide scalable machine learning algorithms that can efficiently handle large datasets distributed across a cluster
	- Enables recommendation, clustering, classification and collaborative filtering using distributed computing frameworks
- What is Spark?
	- Open source distributed computing framework that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance
- What are its goals?
	- Providing unified/ combined framework for big data processing that is efficient and fault- tolerant
	- Highlights include batch processing, real- time streaming, machine learning and graph processing
- What is collaborative filtering?
	- Making predictions about the interests of a user by collecting preferences from other users (collaboration)
	- Analyzes interactions between users and items (e.g. reviews, purchase history) to identify similarities
- What is clustering?
	- Unsupervised machine learning technique that groups similar data points together based on their features
	- Partitions dataset into clusters where the points within the same cluster are similar to each other than those in other clusters
- What is it used for?
	- Discover underlying patterns or structures within data
	- Identifying outliers
	- Segmenting users based on their behavior
	- Grouping points based on their features
- What is the K- Means algorithm used for?
	- Clusters data points into predefined number of clusters
	- Minimizes distance between data points and the centroid of their assigned cluster while maximizing the distance between centroids of different clusters
- How does it work?
	- Iteratively assigns data points to the nearest centroid
	- Updates centroids based on mean of the data points assigned to each cluster
	- Continues until convergence, where centroids no longer change significantly **or** max iterations have been reached
- Optimizations for K- Means?
	- Random Initialization
	- Parallelization 
	- K- Means++ Initialization
- What is classification with respect to machine learning?
	- Supervised machine learning task that categorizes input data into predefined classes or categories
	- Maps input features to output labels, allowing the model to predict the class of unseen data instances
- Describe a few use cases of classification
	- Spam Filtering
	- Sentiment Analysis
- Describe a few models which can be used for classification
	- Logistic Regression
	- Decision Trees
	- Random Forest
	- K- Nearest Neighbors (KNN)
	- Naive Bayes
- Describe the Naive Bayes algorithm
	- Multiclass classification algorithm with "naive" assumption of independence between every pair of features
	- Calculates probability of each class given input features and selects the class with the highest probability as the predicted class
- What is Frequent Pattern Mining (FPM)?
	- Data mining technique used to discover frequently occurring patterns in a dataset
	- Identifies items that often co- occur together in events
- What are its use cases?
	- Recommendation Systems
- Describe the Apriori algorithm
	- Classic algorithm for FPM
	- Iteratively discovers frequent item sets by generating candidate item sets and pruning those that do not meet a minimum support threshold
		- Frequent item sets are extended larger and larger item sets as long as they appear sufficiently often in the database
	- Reduces search space using the Apriori property that any subset of a frequent item set must be frequent

# W12: Streaming Systems

## Streaming Systems

- Why do we need real- time stream processing systems?
	- MapReduce, Hadoop etc. store and process data at scale, but not for real- time systems
	- Hadoop fundamentally has a different set of requirements than batch processing
- What are representative cloud streaming engines?
	- Apache Storm
	- Apache Flink
- What are spouts, bolts, streams and topologies in Apache Storm?
	- Spouts
		- Input source of streams in topology
	- Bolts 
		- Processing container which can perform transformations (e.g. filter, aggregation, join)
	- Streams
		- Unbounded sequence of tuples that is processed and created in parallel
		- Flow of data between spouts and bolts
	- Topologies
		- Network of sprouts and bolts that are connected with stream groupings
- What are the different grouping ways in Storm?
	- Shuffle
		- Picks a random task
	- Fields
		- Consistent hashing on a subset of tuple fields
	- All
		- Sends to all tasks
	- Global
		- Picks task with lowest ID
- What are the three flavors guaranteeing message processing?
	- None
		- Similar to old S4
	- At Least Once
		- Tuple trees, anchoring and spout replay
	- Exactly Once
		- Similar to Hadoop
- Where are they used?
	- Used when needed to ensure different levels of reliability in message processing
	- Exactly once is commonly used when strict message processing semantics are needed
- How does Trident interact with exactly once processing?
	- For Apache Storm, the state storage is left up to the user
	- Trident provides exactly once semantics
		- State is a first- class citizen, but the **exact implementation of state is left up to you**
		- Pre- built connectors to various NoSQL stores like HBase exist for state storage
		- Provides a high level API 
- What is the structure of a Storm cluster?
	- Clojure
	- Java
- How does Storm guarantee fault tolerance?
	- Storm achieves fault tolerance through worker and supervisor failover, tuple tracking and event replay
	- ZooKeeper is used for distributed coordination, maintaining `acks` and tracks the processing state of tuples
- How is Thrift used in Storm?
	- Communication between Nimbus and supervisor nodes
	- Defines binary protocol for serialization and RPC communication
	- Allows Storm to be used by many languages
- What is the `IScheduler` and multi- tenant scheduler in Storm?
	- `IScheduler` is responsible for scheduling topologies on the cluster
	- Multi- tenant schedulers are responsible for allowing multiple topologies to share cluster resources efficiently
- What is stateful stream processing?
	- Maintaining state across multiple data stream events
	- Allows for processing logic where the output depends on the entire history of the input stream
- What is the difference between Storm streaming and Spark streaming?
	- Storm streaming is a real- time stream processing system with low latency and support for complex event processing
	- Spark streaming is a microbatch processing engine that processes data in batches, providing both batch and stream processing features
- What are the advantages and disadvantages of microbatch in Spark streaming?
	- Advantages
		- Rich ecosystem of big data tools
		- Fault tolerance
		- East of programming
		- Integration with existing batch processing workflows
		- Spark SQL
		- Spark ML
		- Spark GraphX
		- SparkR
	- Disadvantages
		- Technically not really streaming
		- Increased latency due to batching
		- Potentially higher resource overhead
- What applications are more suitable for Spark streaming?
	- Applications that require high throughput and fault tolerance
- What is Lambda architecture in stream processing?
	- Hybrid approach to stream processing that combines both batch and stream processing layers to handle both real- time and historical data
	- Batch processing handles failures well
- What is Kappa architecture in stream processing?
	- Approach to stream processing that focuses solely on stream processing
	- Uses a single stream processing layer to handle both real- time and historical data, leveraging immutable event logs for data storage
- Why do we need Lambda and Kappa architecture?
	- Lambda and Kappa addresses tradeoffs between latency, accuracy and complexity in stream processing
	- Lambda
		- Real- time and batch processing
		- Difficult to manage
	- Kappa
		- Simple architecture by focusing only on stream processing
		- Lack key features of batch processing
- What are the general steps in a streaming ecosystem?
	- Gather the data (Funnel)
	- Distributed Queue
	- Real- Time Processing
	- Semi- Real- Time Processing
	- Real- Time OLAP

# W13: Virtualization & Containers

## Virtualization

- What is virtualization?
	- Creating a virtual resource (e.g. VM, virtual storage, virtual environments) that mimics the behavior of physical hardware and distributing it
- What is its main idea?
	- Creation of distributed computing models without creating dependencies on physical resources
	- Abstraction of physical hardware resources combined with instances that are isolated from one another, allows for better resource utilization, increased flexibility, easier management and better scalability 
- User & Kernel Modes
	- Modes designed to isolate processes from each other in the OS
	- User
		- User processed operation in user mode
		- When user application requests a service from the OS, or an interrupt occurs, or a system call is made, there will be a transition from user to kernel mode to fulfill requests
	- Kernel
		- When the system boots, the hardware starts in kernel mode
		- Privileged instructions which execute **only** in kernel mode
			- If user attempts to run privileged instructions in user mode, it will treat the instruction as an illegal operation and traps it to the OS
- CPU Privilege Levels
	- Also known as protection rings or modes, to control access to system resources
		- Ring 0
			- Highest privilege
			- Kernel Mode + OS
		- Ring 3
			- Lowest privilege
			- User Mode
- What are differences between different types of virtualizations?
	- Emulation
	- Full
		- Software
			- Binary Translation
			- Paravirtualization
		- Hardware Assisted
	- MicroVMs
	- OS
		- Containers
- What is the definition of each type?
	- Full
		- Guest operating system runs unmodified
		- Virtualization layer intercepts and translated privileged instructions
	- Paravirtualization
		- Requires modification of the guest OS to communicate directly with virtualization layer
		- Improves performance but needs OS support
	- Hardware Assisted
		- Relies on CPU extensions such as Intel VT-X or AMD-V to improve performance and reduce overhead by offloading virtualization tasks to the hardware
	- Containerization
		- Virtualizes the operating system entirely, allowing multiple isolated user space instances (containers) to run on a single host OS
	- Emulation
		- Simulates hardware components and translates instructions from one architecture to another, allowing software written for one platform to run on another
- What are some examples of each type?
	- Full
		- VirtualBox
		- Virtual PC
		- VMWare
	- Paravirtualization
		- Xen
	- Hardware Assisted
		- KVM
		- Hyper-V
	- Containerization
		- Docker
		- Kubernetes
	- Emulation
		- QEMU
- Xen
	- Hypervisor that provides full virtualization and paravirtualization capabilities
	- Allows multiple guest OS to run concurrently on a single physical machine
	- Invasive changes to kernel to run Linux as a paravirtualized guest
	- Domains
		- Control
		- Guest
		- Driver/ Stub/ Service
- Binary Translations
	- Modifies sensitive instructions on the fly to virtualizable instructions
	- Only needs to translate kernel code that is running in ring 0
	- Most code is executed directly onto the CPU and only the code that needs to be translated is actually translated
	- Does not require changes to guest OS kernel
- What is hardware virtualization?
	- Using hardware support to create and manage virtualized environments
	- Offloads virtualization tasks to the CPU, MMU (Memory Management Unit) and other hardware components
- What is its history?
	- 1st Generation
		- 2006
		- Intel VT-X, AMD-V
		- Lacks explicit support for memory virtualization
		- Does not virtualize MMU
	- 2nd Generation
		- AMD RVI (Rapid Virtualization Indexing)
		- Intel EPT (Extended Page Tables)
		- Fixes flaws of 1st Generation
			- No trace- induced exits
			- No context- switch exits
			- No hidden/ true fault exits
			- VMM does not have to allocate memory for shadow page tables, reducing memory usage
	- 3rd Generation
		- 2012 ~ 2013
		- AMD AVIC
		- Intel APICV
		- I/O MMU Virtualization (IOMMU)
			- AMD-VI
			- Intel VT-D

## Containers

- What is operating system- level virtualization?
	- Virtualizing a physical server at the OS level, enabling multiple isolated and secure virtualized servers to run on a single server
- What are examples of OS virtualization?
	- Solaris Containers
	- FreeBSD Jails
	- Linux Containers
		- Linux Vserver
		- OpenVZ
		- Process Container (cgroups)
		- LXC
		- Docker
- What is OS Virtualization?
	- Type of lightweight virtualization
	- Processes "think" that they see a virtual kernel, but are all sharing the same real kernel in reality
	- Kernel acts as a pseudo- hypervisor to ensure that container and virtualization boundaries are not crossed
- What are OS Containers?
	- Supports all of the resource isolation use cases without the overhead and complexity of running multiple kernel instances
- Differences between VM and containers?
	- Hypervisor (VM)
		- Hardware level virtualization
		- One real HW, many virtual HW, many OS
		- High versatility, can run different OS
		- Lower density, performance, scalability
		- Performance overhead mitigated by new hardware features
	- Containers (CT)
		- OS level virtualization
		- One real HW, no virtual HW, one kernel, many user space instances
		- Higher density, natural page sharing
		- Dynamic resource allocation
		- Native performance with little to no overhead
- What are the three building blocks of containers?
	- cgroups
		- Limits and controls the resource usage of containers
	- Namespaces
		- Isolates and virtualizes system resources 
		- Examples
			- Processes
			- Network Interfaces
			- Filesystems
			- User IDs
		- Provides each container with its own personal view of the system
	- Unionfs
		- Layered file system that allows multiple filesystems to be stacked on top of each other, enabling efficient storage and sharing of container images, as well as file system changes

## Docker

- What is the Union File System?
	- Backbone of container images
	- Stackable unification file system which merges several directories (branches) while keeping physical content separate
	- Overlays several directory into a single mount point
- What are Docker images?
	- Container image that is made of a stack of immutable or read- only layers
	- Used to create Docker containers, contains everything needed to run an application
	- Built using a Dockerfile, which specifies the instructions for assembling the image layer by layer
- Describe Docker architecture and its components
	- Docker Engine
		- Core component responsible for running and managing Docker containers
		- Includes `dockerd` (daemon), which listens for Docker API requests
		- Includes Docker CLI (docker), which is used to interact with the daemon
	- Docker Registry
		- Centralized repository for storing and distributing Docker images
		- Default is Docker Hub but private registries can be used
	- Docker Container
		- Instance of Docker image that runs in isolation with its own filesystem, network and process space
	- Docker Client
		- CLI interface used to interact with Docker Engine and manage containers, images, networks, volumes and other Docker objects
- What are container network models?
	- Formalizes steps required to provide networking for containers while providing an abstraction that can be used to support multiple network drivers
	- Defines how containers communicate with each other and with external networks
	- Examples
		- Bridge
			- Uses Linux bridging and `iptables` to provide connectivity for containers
			- Creates single bridge called `docker0` and attaches a `veth pair` between the bridge and every endpoint
		- Overlay
			- Networking that can span multiple hosts using overlay network encapsulations such as VXLAN
				- Enables Swarm services to communicate with each other
		- Host
			- For standalone containers, remove network isolation between the container and the Docker host and use the host's networking directly
		- `macvlan`
			- Allows you to assign a MAC address to a container, making it appear as a physical device on your network
				- Docker daemon routes traffic to containers by their MAC addresses
	- Overview
		- Sandbox
			- Contains configuration of a container's network stack, includes management of the container's interfaces, routing table and DNS settings
		- Endpoint
			- Joins a sandbox to a network
		- Network
			- Group of endpoints that are able to communicate with each other directly

# W14: Container Orchestration & Docker Swarm

## Docker 

- What is Dockerfile?
	- Imperative method to create a Docker image
- How do you use it?
	- Create a Dockerfile in your project directory and define the required instructions 
	- Build a Docker image using `docker build`, which executes the instructions in the Dockerfile 
- What is Docker Swarm?
	- Clustering and orchestration tool for managing a cluster of Docker hosts and running containerized applications at scale
	- High availability, load balancing and automatic scaling of containers across multiple nodes
	- `ingress` handles the control and data traffic related tasks in Swarm Services
- What are some examples of Swarm Services?
	- Containerized application that needs to be deployed and managed in a clustered environment
		- Web Servers
		- Databases
		- Microservices
		- API Gateways
- What are states of Swarm Services?
	- `NEW`
	- `PENDING`
	- `COMPLETE`
- What are the different types of Docker networks?
	- Bridge
	- Host 
	- Overlay
	- `macvlan`
- What are its usages?
	- Enable communication between containers running on the same host or across multiple hosts in a Docker Swarm cluster
	- Provides networking isolation, security and flexibility for containerized applications
- What is service discovery in Docker Swarm?
	- Method in which containers within a Swarm cluster can discover and communicate with each other dynamically
	- Docker Swarm has built- in service discovery and DNS- based service discovery mechanisms to facilitate communication between services
- What are the three ways to map hosts to containers?
	- Bind Mount
		- Specific directory or file on the host is mounted onto a container
		- Changes made to the directory or file on either the host or the container are immediately reflected in the other
	- Volume
		- Managed by Docker and persists data outside the lifecycle of a container
		- Can be shared and reused among multiple containers
		- Provides better performance and reliability compared to bind mounts
	- `tmpfs`
		- Temporary file storage system that is located in the memory, not on the disk
		- Allows you to mount a portion of the host's memory as a filesystem within a container, providing fast access to temporary files and data
		- Useful for storing temporary data that does not need to persist across container restarts
- What is Docker Compose?
	- Allows you to define and run multi- container Docker applications 
	- Can define application environments using YAML and manage the lifecycle with simple commands
- What is its usage?
	- Simplifies the process of managing multi- container applications by providing declarative syntax for defining services, their dependencies, volumes, networks and environment variables

# W15: Container Orchestration & Kubernetes

## Kubernetes

- What is Kubernetes?
	- Container orchestration platform that automates the deployment, scaling and management of containerized applications
	- Eliminates the need for worrying about underlying infrastructure
- What is the function of Kubernetes?
	- Automates the deployment, scaling and management of containerized applications
- What are the primary advantages of using Kubernetes?
	- Scalability
		- Automatic scaling based on demand
	- High Availability
		- Automatically restarts failed containers or rescheduling them on healthy nodes
	- Portability
		- Provides consistent environment for deploying and running applications across different infrastructure providers & environments
	- Automation
		- Automates aspects of application deployment and management, reducing manual intervention & error
	- Extensibility
		- Can be integrated with other tools and platforms through its API
- What is the architecture of Kubernetes?
	- Master
		- API Server
		- Controller
		- Scheduler
		- `etcd`
	- Worker
		- Kubelet
		- `kube-proxy`
		- `container-runtime`
- What are the components of master nodes and worker nodes?
	- Master
		- Manages cluster and coordinates communication between components
	- Worker
		- Hosts the running application
- What are three pod container design patterns?
	- Sidecar
		- Made up of two containers
			- Application
			- Sidecar
				- Augments and improves application container without its knowledge
		- Example Use cases
			- Adding HTTPS to legacy service
			- Dynamic configuration with sidecars
	- Ambassador
		- Brokers interactions between the application container and the rest of the world
		- Example Use Cases
			- Sharding a service
			- Service discovery
			- Experiments or request splitting
	- Adapter
- Describe the building blocks of Kubernetes
	- Nodes
		- Physical or virtual machines that run containerized applications managed by Kubernetes
	- Pods
		- Smallest deployable units in Kubernetes, consists of one or more containers that share network and storage resources
	- Labels
		- Key- value pairs attached to objects (e.g. pods) for identifying and grouping related resources
	- Selectors
		- Queries used to select objects based on their labels for various operations
	- Controllers
		- Control loops that manage the lifecycle of Kubernetes objects
- What is the Kubernetes Service?
	- Abstraction which defines a logical set of pods running somewhere in your cluster, that all provide the _same_ functionality
	- Sometimes referred to as a microservice
- What does it do?
	- Exposes a set of pods as a network service, allowing other applications to communicate with them reliably
	- Provides load balancing, service discovery and automatic failover for pods
- What is the Kubernetes Service Proxy?
	- `kube-proxy` is a network proxy that runs on each node in a Kubernetes cluster
	- Maintains network rules and performs connection forwarding for Kubernetes services, allowing external clients to access services running on that cluster
- Differences between Kubernetes and Docker Swarm?
	- Both are container orchestration platforms
	- Kubernetes offers features for managing large scale containerized applications
	- Docker Swarm is simpler to use and is suitable for smaller scale deployments
	- Kubernetes can run on many different platforms, Docker Swarm needs to be used with Docker Engine and Docker containers

# W16: Future Developments in the Cloud

## Thirteen Predictions

- Adoption of cloud computing will grow rapidly
- Cloud will become more global
- Regulated industries will move to the cloud
- Storage capacity will continue to increase rapidly
- SSD's share of the market will grow, but there will still be a lot of HDD and magnetic tape in use
- Cloud will continue to support AI
- There is a symbiotic/ mutually- beneficial relationship between AI and clouds
- Data center accelerator's market growth will continue to skyrocket
- Future of cloud computing access is mobile
- FaaS will continue to grow in popularity and use
- Low- code/ No code/ citizen development will become increasingly important to the industry as it is a way to help deal with the predicted 1 million software developer shortfall in the US
- Increased adoption of clouds means increased risks of security breaches
- IoT will grow and fuel the growth of the cloud industry
- Hybrid clouds/ multi- clouds/ omni- clouds will become more feasible and widely- used
- Demand for cloud professionals will grow