# Data Science Interview Questions [^1]

[^1]: Translated and modified for ease of access, soure courtesy of [@zzsza](https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/)

## Common

- Why did you apply for this position?
- Why did you apply to our company?
- What do you think is the merit/ attraction/ charm of this job?
- What are your strengths in relation to this job?
- What is the goal you want to achieve while working in this job?
- What kind of work/ effort did you put in for this job?
- Why should we recruit/ hire you?
- What are your strengths and weaknesses?

## Project

- How did you get the data?
- Why did you use this algorithm for the project?
- Is there a similar algorithm to the one that you used?
- What are the disadvantages of this algorithm?
- What kind of work did you do for this project?
- What did you learn from this project?
- What would you do differently if you were to restart this project?

## Math/ Statistics

- What are **eigenvalues** and why are they important?
- What are **eigenvectors** and why are they important?
- Explain **sampling** and **resampling**.  
  - What are the benefits of resampling?
- What are **probability models** and **random variables**?
- What are **cumulative distribution functions** and why are they important?
- What are **probability density functions** and why are they important?
- What are **Bernoulli distributions** and why are they important?
- What are **Gaussian distributions** and why are they important?
- What are **Bayesian distributions** and why are they important?
- What are **binomial distributions** and why are they important?
- What are **polynomial distributions** and why are they important?
- What are **t distributions** and why are they important?
- What are **F distributions** and why are they important?
- What are **chi- square distributions** and why are they important?
- What are **Beta distributions** and why are they important?
- What are **Gamma distributions** and why are they important?
- What is **conditional probability**?
- What is **covariance**?
- What is a **correlation coefficient**?
- What is a **confidence interval**?
- Explain the concept of a **p- value** to a customer.
- Is p- value still valid in this day and age?
  - When does p- value tend to mislead people?
- How do we decide what tests (e.g. A/B test) we will conduct and determine whether or not it is statistically significant?
- Why is **$R^2$** important?
- When should we use mean and when should we use median?
- What is the **central limit theorem** and why is it important?
- What is **entropy** and why is it important?
- What is **information gain** and why is it important?
- What is **information loss** and why is it important?
- In the era of "big data", are normality tests still relevant?
- When should you use **parametric** methodology?
- When should you use **non- parametric** methodology?
- What is the difference between **likelihood** and **probability**?
- For an extremely small sample size, how can we establish a predictive model?
- What are the key differences between **Bayesian** statistical approaches and **Frequentist** statistical approaches?
- What is **statistical power** and why is it important?
- If there is a missing value, how should you deal with it?
- What is the criteria for judging outliers?
- How do you calculate the appropriate sample size that you need?
- How do you control **bias**?
- In what cases are **logarithmic functions** useful?

## Analysis

- What does it mean for a feature to be "good"?
  - What are some ways to judge the performance of a feature?
  - What are some ways to select a feature?
- What does "correlation doesn't equate to causation" mean?
- What are advantages/ disadvantages of the A/B test and how do we mitigate them?
- You need to create a model that predicts what products customers will buy tomorrow.
  - Choose which techniques (e.g. SVM, Random Forest, Logistic Regression) you will use and explain this to a practitioner who has no statistical & machine learning background.
- What are some ways to deal with **high dimensional data**, expecially when dealing with clustering against hundreds of features?

## Machine Learning

- What is **cross validation** and how do you perform it?
- What is the right metric for **regression**?
- What is the right metric for **classification**?
- Give examples of metrics.
  - (e.g. **RMSE**, **MAE**, **precision**, **recall**, **F- score**)
- Why do we need to normalize?
  - How do we normalize data?
- What is **local minima**?
- What is **global minima**?
- Explain what the **Curse of Dimensionality** is.
- Give examples of **dimensional reduction** techniques.
  - **PCA** is a dimensional reduction technique, a data compression technique and a noise cancellation technique.
  - Explain why.
- What is the relationship between **LSA**, **LDA** and **SVD**?
- What is a **Markov chain**?
- Why does SVM project the data onto higher dimensions?
  - Explain why this makes SVM suitable for high dimensionality work.
- Explain how **Naive Bayes** compares to modern machine learning techniques.

## Deep Learning

- X

### Basic Deep Learning

- X

### Computer Vision

- X

### Natural Language Processing

- You need to extract a topic from a data dump.
  - Explain how you would approach this task.

### Reinforcement Learning

- X

### General Adversarial Networks

- X

## Recommendation Services

- X

## Database

- X

## Data Visualization

- X

## System Engineering

- X

## Distributed System Processing

- X

## Web Architecture

- X
